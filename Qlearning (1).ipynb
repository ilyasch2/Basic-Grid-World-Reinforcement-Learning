{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Grid World Reinforcement Learning\n",
        "\n",
        "Welcome to the \"Basic Grid World Reinforcement Learning\" project notebook! In this project, we'll explore a basic grid world environment and implement reinforcement learning techniques to train an agent to navigate this world.\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "- **Objective**: Train an agent to learn optimal strategies for moving through a grid world, avoiding obstacles, and reaching predefined goals.\n",
        "\n",
        "- **Techniques**: We'll use Q-learning, a popular reinforcement learning algorithm, to achieve our goal.\n",
        "\n",
        "- **Implementation**: Our project is organized into classes, making it easy to understand and modify different components.\n",
        "\n",
        "## Notebook Sections\n",
        "\n",
        "This notebook is divided into several sections:\n",
        "\n",
        "1. **Environment Setup**: We'll define the grid world environment and the agent's actions.\n",
        "2. **Agent Training**: We'll implement the Q-learning algorithm and train the agent.\n",
        "3. **Visualization**: We'll visualize the agent's behavior and training progress.\n",
        "4. **Customization**: You can customize the grid layout and experiment with different configurations.\n",
        "5. **Conclusion**: We'll summarize the project's key takeaways and potential future improvements.\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "Before we begin, make sure you have the required dependencies installed.\n",
        "\n",
        "Let's get started with the environment setup and training the agent!\n",
        "\n"
      ],
      "metadata": {
        "id": "ySpiyfjotUH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import randint, seed\n",
        "\n",
        "class EnvGrid(object):\n",
        "    \"\"\"\n",
        "    docstring for EnvGrid.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(EnvGrid, self).__init__()\n",
        "\n",
        "        self.grid = [\n",
        "            [0, 0, 1, 0, -1],\n",
        "            [0, -1, 0, 0, 0],\n",
        "            [0, 0, 0, -1, 0],\n",
        "            [0, -1, 0, 0, 0],\n",
        "            [1, 0, 0, -1, 0]\n",
        "        ]\n",
        "\n",
        "        # Starting position/ Here we start from the bottom left of the grid\n",
        "        self.y = 4\n",
        "        self.x = 0\n",
        "        # Define goals and obstacles, we can also use create_random_grid bellow to initialize our goals position randomly\n",
        "        self.goals = [(0, 2), (4, 0), (4, 4)]\n",
        "        self.obstacles = [(0, 4), (1, 1), (2, 3), (3, 1), (4, 3)]\n",
        "        # Dimensions of the grid\n",
        "        self.num_rows = len(self.grid)\n",
        "        self.num_cols = len(self.grid[0])\n",
        "        # Possible actions\n",
        "        self.actions = [\n",
        "            [-1, 0],  # Up\n",
        "            [1, 0],   # Down\n",
        "            [0, -1],  # Left\n",
        "            [0, 1]    # Right\n",
        "        ]\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the agent's position and return the initial state.\n",
        "        \"\"\"\n",
        "        self.y = 4\n",
        "        self.x = 0\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"\n",
        "        Convert the current position to a state representation.\n",
        "        \"\"\"\n",
        "        return self.y * self.num_cols + self.x\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take an action in the environment and return the next state and reward.\n",
        "        \"\"\"\n",
        "        self.y = max(0, min(self.y + self.actions[action][0], 4))\n",
        "        self.x = max(0, min(self.x + self.actions[action][1], 4))\n",
        "\n",
        "        state = self.get_state()\n",
        "\n",
        "        reward = 0\n",
        "        if (self.y, self.x) in self.goals:\n",
        "            reward = 1\n",
        "        elif (self.y, self.x) in self.obstacles:\n",
        "            reward = -1\n",
        "\n",
        "        return state, reward\n",
        "\n",
        "    def create_random_grid(self, num_goals=2):\n",
        "      \"\"\"\n",
        "      Create a random grid with a specified number of goals.\n",
        "      \"\"\"\n",
        "      grid = [[0] * 5 for _ in range(5)]\n",
        "      obstacles = [(0, 4), (1, 1), (2, 3), (3, 1), (4, 3)]\n",
        "\n",
        "      self.goals = random.sample([(i, j) for i in range(5) for j in range(5)], num_goals)\n",
        "\n",
        "      for g in self.goals:\n",
        "          grid[g[0]][g[1]] = 1\n",
        "\n",
        "      for o in obstacles:\n",
        "          grid[o[0]][o[1]] = -1\n",
        "      self.grid= grid\n",
        "\n",
        "\n",
        "    def show(self):\n",
        "        \"\"\"\n",
        "        Display the current state of the grid.\n",
        "        \"\"\"\n",
        "        print(\"---------------------\")\n",
        "        for y in range(self.num_rows):\n",
        "            for x in range(self.num_cols):\n",
        "                print(\"%s\\t\" % ( \"X\" if y == self.y and x == self.x else \"G\" if (y, x) in self.goals else self.grid[y][x]), end=\"\")\n",
        "            print(\"\")\n",
        "\n",
        "    def is_finished(self):\n",
        "        \"\"\"\n",
        "        Check if the agent has reached a goal state.\n",
        "        \"\"\"\n",
        "        return (self.y, self.x) in self.goals\n",
        "\n",
        "def take_action(st, Q, eps):\n",
        "    \"\"\"\n",
        "    Choose an action using an epsilon-greedy strategy.\n",
        "    \"\"\"\n",
        "    if random.uniform(0, 1) < eps:\n",
        "        action = randint(0, 3)\n",
        "    else:  # Or greedy action\n",
        "        action = np.argmax(Q[st])\n",
        "    return action\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Uncomment the following line if you want to use a fixed random seed for reproducibility\n",
        "    #seed(0)\n",
        "    env = EnvGrid()\n",
        "    env.create_random_grid()\n",
        "\n",
        "    st = env.reset()\n",
        "\n",
        "    num_states = env.num_rows * env.num_cols\n",
        "    Q = np.zeros((num_states, 4))\n",
        "\n",
        "    learning_rate = 0.1\n",
        "    discount_factor = 0.9\n",
        "    num_episodes = 10000\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        st = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        iteration = 0  # Initialize the iteration counter\n",
        "\n",
        "        while not done:\n",
        "            at = take_action(st, Q, 0.4)\n",
        "            stp1, r = env.step(at)\n",
        "            atp1 = take_action(stp1, Q, 0.0)\n",
        "            Q[st][at] = Q[st][at] + learning_rate * (r + discount_factor * Q[stp1][atp1] - Q[st][at])\n",
        "            st = stp1\n",
        "\n",
        "            total_reward += r\n",
        "\n",
        "            done = env.is_finished()\n",
        "    st = env.reset()\n",
        "\n",
        "    # Render the agent's behavior on the random grid\n",
        "    while not env.is_finished():\n",
        "        env.show()\n",
        "        #print(env.y, env.x)\n",
        "        #print(env.is_finished())\n",
        "        at = take_action(st, Q, 0.1)  #10% Exploration\n",
        "        #print(at)\n",
        "        stp1, r = env.step(at)\n",
        "        st = stp1\n",
        "    env.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY0sIGLv_ixG",
        "outputId": "38403144-4722-4f11-f13d-353ff35098db"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "0\t0\t0\t0\t-1\t\n",
            "0\t-1\tG\t0\t0\t\n",
            "0\t0\t0\t-1\tG\t\n",
            "0\t-1\t0\t0\t0\t\n",
            "X\t0\t0\t-1\t0\t\n",
            "---------------------\n",
            "0\t0\t0\t0\t-1\t\n",
            "0\t-1\tG\t0\t0\t\n",
            "0\t0\t0\t-1\tG\t\n",
            "X\t-1\t0\t0\t0\t\n",
            "0\t0\t0\t-1\t0\t\n",
            "---------------------\n",
            "0\t0\t0\t0\t-1\t\n",
            "0\t-1\tG\t0\t0\t\n",
            "X\t0\t0\t-1\tG\t\n",
            "0\t-1\t0\t0\t0\t\n",
            "0\t0\t0\t-1\t0\t\n",
            "---------------------\n",
            "0\t0\t0\t0\t-1\t\n",
            "0\t-1\tG\t0\t0\t\n",
            "0\tX\t0\t-1\tG\t\n",
            "0\t-1\t0\t0\t0\t\n",
            "0\t0\t0\t-1\t0\t\n",
            "---------------------\n",
            "0\t0\t0\t0\t-1\t\n",
            "0\t-1\tG\t0\t0\t\n",
            "0\t0\tX\t-1\tG\t\n",
            "0\t-1\t0\t0\t0\t\n",
            "0\t0\t0\t-1\t0\t\n",
            "---------------------\n",
            "0\t0\t0\t0\t-1\t\n",
            "0\t-1\tX\t0\t0\t\n",
            "0\t0\t0\t-1\tG\t\n",
            "0\t-1\t0\t0\t0\t\n",
            "0\t0\t0\t-1\t0\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XkqygDERsxeM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}